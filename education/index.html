---
layout: default
title: Project 1
---
<div class="blurb">
	
	<h1>Project 1 <small>Evidence Portfolio</small></h1>
	<br>

	<h2>User Interaction with Google Cardboard</h2>
	
	<p>
	To learn how to develop for the Google VR platform on Android, we made a <a href="https://github.com/OtagoPolytechnic/CommSoftTasks/tree/Production/VrPanoViewer">New Zealand themed demo application</a>. The app lets users 		view a collection of 360 images from New Zealand settings, displayed in VR mode to be compatible with mobile based VR headsets. 	It was a challenge to design methods of incorporating interactivity into the project.
	Previous projects I have worked on using mobile devices have used the touch screen, as that is the main method of user 	interaction on the platform. But the user cannot touch the screen when their device is inside the headset, so we had to explore alternate means.
 	<br><br>
	I implemented user interaction by having the application change to the next image when the user looks at their feet. To create this functionality I used the accelerometer built into mobile devices. This records the acceleration in each of the 3 axes. When the device is not accelerating relative to the ground, the force of gravity can be used to calculate the orientation of the device. In the context of a VR headset, this means you can determine the direction the user is looking.
	<br> 
	When the device is parallel to ground the force of gravity is aligned with Z axis of the accelerometer. So the Z value given by the accelerometer will be 9.8 or -9.8 depending on if the device is facing screen up or screen down(assuming the device is not otherwise accelerating). When the Z value is greater than 9 the image is changed. This happens when the device is parallel to the ground, screen up. In terms of the VR headset, this means the user is looking towards the ground. The user has to look up again as a reset mechanism, to prevent the image changing multiple times unintentionally.
	<br><br>
	<img src="/assets/img/accel.PNG">
	<br><br>	
	I chose to have to action mapped in this way because I wanted it to be difficult to trigger the action accidently, but easy to do intentionally. I thought the place the user is least likely to want to look is directly at the ground. Also looking at your feet is a simple instruction to communicate to users and easy for them to understand. I had considered mapping some actions to when the user looks directly upwards, or having left and right head tilts to let the user go back and forwards through the image collection. but I didn’t want to incorporate too many gestures as input actions, as this would result in them being falsely detected when the user is just looking around.
	<br><br>
	The demo app also used voice recognition for input. To implement this we used the android.speech.RecognitionListener interface and the onResults event. This event is triggered when the voice prompt recognises a result string. This is a good method for user interaction for VR apps as it does not require direct contact with the device. Different words can be mapped to different actions, making voice commands very versatile. The weakness is that the command can easily be lost among background noise, which in some cases could result in the wrong action being triggered. Also Android voice recognition is only available through an internet connection. But I consider these to be acceptable faults for the functionality offered.
	<br><br>
	<img src="/assets/img/speech.PNG">
	<br><br>
	For the prototype the command “next” can be used to move to the next image. Voice listening was activated when the image was changed. This was unintuitive and clumsy for users. For a more robust execution I would want a dedicated action that activates the voice prompt. Such as using the looking down gesture to activate the voice prompt,  allowing for a more complex control scheme built around speech recognition.
	</p>
	
	
	<br><br>
	<h2>Magnetic Button</h2>
	
	<p>
	A technical challenge was to implement user interaction using the magnetic button on the virtual reality image viewer app. This is an input device built into the Google Cardboard headset.
	<br>
	<table>
		<tr> <td><img src="/assets/img/mag1.jpg" width="100%"></td> 	    <td><img src="/assets/img/mag2.jpg" width="100%"></td> </tr>
		<tr> 
			<td>On the outside of the headset is magnetic disc. It sits in a long slot so the user can shift its position.</td> 
			<td>On the corresponding position inside the headset is another magnetic disk. This sits in a fitted slot so that it does not shift around.</td>
		</tr>
	</table>
	<br>
	<p>
	We began by testing the button using the Treasure Hunt sample app that comes with the Google VR SDK for Android. In this early testing the button was very inconsistent and would often not register the input.
	We learnt that the button functions use the phone’s inbuilt magnetic field sensor, and began attempts to implement the button in our app. From monitoring the value of the magnetic field sensor we could see the effect of the button. This also showed that the magnetic field was greatly influenced by factors other than the position of the button. The device can easily shift around while in the headset, the user could place the device in the inverted orientation, and different devices have the sensors in different positions. All these factors affect the magnetic field sensor, so the absolute value of the sensor is not an entirely accurate predictor of the position of the button.
	Our implementation of the button checked the value of the magnetic sensor against two constants associated with the button being in the up and down positions. This worked well for demonstration but because of the variance in the magnetic field, would not be a good long term solution. We decided to design a system to calibrate the magnetic field, and base the button handling logic of those values. Allowing the app to counteract the variance in the sensor read value.
	Designing the calibration system was a very unique problem. The system had to communicate to the user when to push and release the button, and when the process was complete. The device had to be in the headset to calibrate the button, so the user cannot see or touch. This greatly restricted the tools we had available to design this system. Building this system required finding new design solutions than what I had used in my previous work with mobile development.
	<br>
	I learned to consider the platform I am developing for, when designing projects. The problem of building the calibration system initially appeared simple, because I was not considering the strengths and weaknesses of the VR platform
	</p>
	
	
	<br><br>
	<h2>Android Logic Timing</h2>
	
	<p>
	My project team was tasked with building <a href="https://github.com/OtagoPolytechnic/CommSoftTasks/tree/Production/VisScanAndroid">suite of tools to aid people with visual impairment</a>. The tools of this program involve moving graphics across the display to test the user for various forms of vision impairment. This task presented the technical challenge of running the update logic for the program on a timer loop. I needed to build a system for moving on-screen graphics used in the test tools. This was a challenge because the user interface can only be updated on the main thread, and the timer runs on a separate thread. I also wanted the architecture to be easily extendable so I could add additional test tools.
		<br>
	I started by having each fragment handle its own timing. This meant each tool could have very specialised and efficient timing mechanisms. The cost of this is that I had to duplicate code to build the timing system for each tool. And the lack of uniformity in the tools made it clumsy to integrate multiple tool fragments into the main system, as I needed the application to switch between and run the multiple tool fragments.
		<br>
	I made an interface IVisualTest for test tool fragments. This let me use the generic test tool function calls in the outer logic control on the MainActivity. 
	<br><br>
	The methods I included in the interface are:
	</p>
	
	<table>
		<tr> <td>Run()</td> 	    <td>The main logic loop of the tool. To be called on every update cycle</td> </tr>
		<tr> <td>ToCSV()</td> 	    <td>Print the test results to a csv formatted string, for saving results</td> </tr>
		<tr> <td>GetTestType()</td> <td>Return a unique identifier string for the tool, for file naming purposes</td> <t/r>
	</table>
	<br>
	<p>
	The solution I implemented was to have a shared timer on the MainActivity that drives the logic of the application. The current tool’s fragment is stored in the testFrag variable of type IVisualTest Each cycle the timer thread is paused for a preset delay, then I use runOnUiThread() to run current fragment’s Run() function. This makes the outer logic loop generic across all the different test tools. This system is very scalable, I can easily change the current test tool and add new tools without modifying the main timer loop. 
	<br><br>	
	<img src="/assets/img/loop.PNG">
	<br><br>
	</p>
			
	
	<br><br>
	<h2>Filtering Django JSON Output</h2>
			
	<p>
	My team had the task of building an <a href="https://github.com/OtagoPolytechnic/CommSoftTasks/tree/Production/BotanicalGardens_Django_API/mysite">application for the local botanical gardens</a>, to store and present information on their plant collection. The end product will consist of front end mobile application to present information the the user. As well as a back end web service to manage the database containing the plant data. This semester we began working with the Django Rest Framework as a possible platform for the back end database.
	<br><br>
Django is a python framework for building web applications. Django uses the model, view, controller architecture. 
	<br>
The Django Rest Framework is a toolkit for building restful web APIs in Django.
	<br>
Using the Django Rest Framework, we built a web API to deliver data from the botanical gardens database in the JSON format. This service could then be utilized by front end applications to present information to the end user.
	
	</p>
			
	<br>
	
	<p>The data is stored as model classes. Our application stored the data of individual plants in the Item class.</p>
	<img src="/assets/img/model.PNG">
	<br><br>
			
	<p>The serializer class converts the model from it's python class to a JSON string</p>
	<img src="/assets/img/serializer.PNG">
	<br><br>		
	
	<p>A view uses the model and the serializer to form a webpage for the user</p>
	<img src="/assets/img/itemList.PNG">
	<br><br>		
	
	<p>The views are mapped onto urls in the urls python file</p>
	<img src="/assets/img/url.PNG">
	<br><br>
			
	<p>The user can send a url request to get data from the web server</p>
	<img src="/assets/img/json.PNG">
	<br><br><br>
			
	<p>
	   Our web API needed to provide search functionality so the user could query the database and receive a response containing data on matching plants. In order to provide this functionality we needed to be able to filter the JSON text output.
		<br>
		There are several methods of filtering available in the Django Rest framework. I had to learn the purpose of each filter and evaluate what filter to use for the front end functionality I wanted.
   	</p>
	<br>
	<p>
	We implemented filtering against the url for primary key matches. This allowed us to implement a details view by returning the single entry for a given primary key.
		<br>
The client sends a request containing the primary key of the item they wish to retrieve the data for.  This call the ItemDetail view, returning the JSON string for the item entry associated with that primary key.
		<br><img src="/assets/img/details.PNG">
		<br>
This is a useful feature for allowing the front end application to easily retrieve the information on a single entry.	
	</p>
			
	<br><br><br>
	<p>
	We also used the Django search filter. This is a built in function of the Django Restful framework that provides single string input parameter queries. The input string is checked against a set of fields of the model you want to search.
	The Item search view checks the input string against the Item Model’s fields collectionID__collectionName', 'collectionID__collectionDescription', 'siteSpecificDescription', 'speciesID__genName', 'speciesID__sciName', and 'speciesID__genDescription. So the view will return any Item entry where the input string appears in any of these fields.
	<br>
	<br><img src="/assets/img/searchView.PNG"><br>
	<br><img src="/assets/img/search.PNG">
	<br><br>	
	The Django search filter was the next filtering method I implemented after primary key filtering. This method was very easy for me to understand and implement. When I first implemented the search I thought it was too broad to be of much use. I was planning on just using it as a means to learn filtering and as a temporary fill in, and then abandoning it in favour of more complex methods that give more specific results. 
	<br><br>	
	When I started thinking more about the front end applications using the API, I understood the purpose of the search filter. In our demo front we implemented manual search functionality. I think this is the best use for this filter. It wouldn’t be much use in an environment where user interaction is more constrained. 
	<br><br>
	I chose to include all the text fields of each model in their search functions to keep the results broad. That characteristic makes the filter distinct and more useful.
	</p>
			
	<br><br><br>
	<p>
	The API also implemented parameter filtering. This lets us map search terms onto the fields of model you are searching. First we implemented this built directly on top of the list entries view. Then it was moved into a filter class which was much more clean and readable.
	<br><br><img src="/assets/img/ParamFilter.PNG">
	<br><br>
Using the filter class I was able to add filtering on the fields foreign key entries. The syntax for this was confusing to me at 		first because I have not seen the double underscore syntax before. This was used to filter item entries against the common name 	and scientific name of their species. And the name of the collection the item is held in. As these values are not stored 		directly within the item model. It was very important to get this functionality implemented as it made our API much more 		powerful.	
	</p>
	
			
	<br><br>
	<h2>Reflective Statement</h2>	
			
	<p>
	My experience has greatened my understanding on developing large scale projects in a team environment. I have learned to build strong group cohesion and how to manage longer term projects.
 	<br><br>
	Maintaining good communication is imperative to keeping the team moving as a coordinated unit. I had to adjust my mindset from working on small individual tasks. Early on the group was focused on smaller individual tasks, and learning the technologies we would be working with during development. During this time able to get away with weak communication. So I didn’t really bother to keep my team updated on my progress or share resources. It would have helped the team get into a good workflow if we had better communication in the early weeks of the project.
	Not building strong communication early on in the project hurt the group when we moved into working on large group tasks. The first group task we worked on was to build an API using the Django Rest framework. We didn’t coordinate very well and this made the group much less efficient. We divided efforts unnecessarily and people were working separately where we could have worked more cohesively. This introduced the additional task of integrating everyone's individual work into the final product.
	Through my work in project I have developed my ability to effectively communicate in a group work environment. I always try and make sure my team is updated on my progress when I am assigned tasks. I show my work when I meet with the group so everyone has a good idea of what I have done. I also keep updated on the progress of the other members of my team. This reduces the strain on the group when integrating my work into larger projects, or when other team members are tasked to expand on work I have developed. 
	<br><br>
	Forming a good plan has also been an important part of development. I learned to better manage my time spent on work. And to organise with my team to keep working at optimal efficiency.
	<br><br>
	I have learned to better manage my time and plan milestones. I have to plan for setbacks and additional duties that can arise during development. This would often be work for my other classes. When I planned time to set aside to work on my task for the project I would not allow time to work on tasks for my other classes. When my scheduling was thrown off by this I had to rush out work to meet deadlines. And I could not take my time to develop my work as much as I’d have liked. 
	<br><br>
	Another aspect of planning is to organize with your team. When I started project I wasn’t very forward thinking in how I collaborated with the rest of my group. I would work in a similar manner to how how I do my regular work. Also it often seemed that team members were unsure of what tasks they were assigned to. And people took on work that could have been spread across the team. Which doesn’t take full advantage of working with a group. And the poor organisation made it needlessly difficult to deal to team level tasks when they occurred. 
	I learned it is important to have early ongoing communication with my team. Creating a plan of how tasks will be divided amongst the group makes it easier to hold formation during development. When everyone is aware of what their own tasks and what everyone else has been assigned, the group is more able to work as a coordinated unit. Less development time is wasted on management and integrating individual work into the main project. This is a much more efficient way of completing group work.
	</p>	
</div><!-- /.blurb -->
